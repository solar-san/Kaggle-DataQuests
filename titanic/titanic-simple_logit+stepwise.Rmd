---
title: "Machine_Learning_from_Disaster_with_R"
subtitle: "Logit Approach"
author: "Marco Solari"
date: "2023-09-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE
  )

requirements <- c(
  "tidyverse",
  "glmnet",
  "ggthemes"
)

lapply(
  requirements, 
  library, 
  character.only = T)

theme_set(
  theme_tufte(
    base_size = 12,
    base_family = 'Atkinson Hyperlegible',
    ticks = F
    )
  )
```

# Loading data

```{r data loader}
data_loader <- function(filename, dataset, path = "data/") {
  
  path <- paste0(path, filename)
  read.csv(
    unz(
      path,
      dataset
    )
  ) 

}
```

```{r}
train_data <- data_loader(
  "titanic.zip",
  "train.csv"
)
test_data <- data_loader(
  "titanic.zip",
  "test.csv"
)
```

# Data Exploration

```{r}
train_data %>% str
```
```{r}
test_data %>% str
```

Some variables might benefit from scaling: `Fare` and `Age`.

```{r}
train_data <- train_data %>% 
  mutate(
    Age = scale(Age),
    Fare = scale(Fare),
    SibSp = scale(SibSp),
    Parch = scale(Parch)
  )
test_data <- test_data %>% 
  mutate(
    Age = scale(Age),
    Fare = scale(Fare),
    SibSp = scale(SibSp),
    Parch = scale(Parch)
  )
```

```{r}
train_data %>% summary
```

> In the test data we do not have the dependent variable. Therefore, we need to split the dataset and use cross-validation to estimate the error.

```{r}
N_train_obs <- dim(train_data)[1]

train_idx <- sample(
  N_train_obs,
  N_train_obs*.5
)
```

# Data transformation: `as.factor`

```{r}
factorize_data <- function(dataframe) {
  dataframe %>% 
  select(-Name, -Ticket) %>% 
  mutate(
    Pclass = as.factor(Pclass),
    Sex = as.factor(Sex),
    # SibSp = as.factor(SibSp),
    # Parch = as.factor(Parch),
    Cabin = as.factor(Cabin),
    Embarked = as.factor(Embarked)
  )
}
```

```{r}
train_data_factorized <- factorize_data(dataframe = train_data) 
test_data_factorized <- factorize_data(dataframe = test_data)
```

```{r}
train_data_factorized %>% summary
```

# Data Visualization

```{r, eval=FALSE, include=FALSE}
train_data %>% 
  factorize_data() %>%
  ggplot(
    aes(
      x = Age,
      fill = factor(Survived)
    )
  ) +
  geom_density(
    alpha = .7,
    na.rm = T
  ) +
  geom_vline(
    xintercept = quantile(
      train_data$Age,
      c(
        .25, 
        .5, 
        .75
        ),
      na.rm = T
      ), col="white",
    lwd=1
    ) +
   scale_color_viridis_d(
    aesthetics = c(
      "colour", 
      "fill"),
    option = "F"
  ) +
  facet_wrap(
    ~factor(Pclass)
  ) +
  labs(
    fill = "Survived"
  ) +
 
  theme(
    legend.position = "top"
  )
```

```{r}
qq_gender <- train_data %>% 
  factorize_data() %>% 
  group_by(
    Sex,
    Pclass
    ) %>% 
  reframe(
    Age_Quantiles = quantile(Age, probs = c(.25, .5, .75), na.rm = T)
  ) %>% 
  mutate(
    Quantile = rep(
      c(
        .25, 
        .50, 
        .75
        ),
      6
    )
  )
```

```{r}
qq_gender %>% 
  filter(Quantile == .5)
```


```{r}
train_data %>% 
  factorize_data() %>%
  ggplot(
    aes(
      fill = factor(Survived),
      x = SibSp,
      y = Age/100
    )
  ) +
  geom_bar(
    width = .4,
    stat = "identity"
  ) +
   geom_hline(
    yintercept = quantile(
      train_data$Age,
      c(
        .25, 
        .5, 
        .75
        ),
      na.rm = T
      ), col="grey40",
    lwd=.3,
    alpha = .4
    ) +
  facet_wrap(
    vars(
      Sex,
      Pclass
      )
  ) +
  scale_color_viridis_d(
    aesthetics = c(
      "colour", 
      "fill"),
    option = "G",
    # begin = 1,
    # end = 0
  ) +
  labs(
    fill = "Survived",
    x = "Number of Siblings",
    y = "Age (Normalized)"
  ) +
  theme(
    legend.position = "top"
  )
```


# Model selection: Logistic Regression + Stepwise Selection

```{r}
fit_full <- glm(
  Survived ~ ., 
  data = train_data_factorized, 
  family = "binomial",
  na.action = na.omit,
  subset = train_idx
)
```

```{r}
fit_full %>% summary
```

```{r}
fit_null <- glm(
  Survived ~ 1, 
  data = train_data_factorized, 
  family = "binomial",
  na.action = na.omit,
  subset = train_idx
)
```

```{r}
sel_stepwise <- step(
  fit_full,
  scope = c(fit_null, fit_full),
  direction = "both",
  k = log(
    dim(train_data_factorized)[1]
    )
)
```


```{r}
sel_stepwise %>% summary()
```

```{r}
sel_stepwise <- update(
  sel_stepwise,
  formula = Survived ~ Pclass*Sex*Age
)
sel_stepwise %>% summary()
```

# Make Predictions and Submission `csv`

```{r}
predictions <- 
  ifelse(
  predict(
    sel_stepwise,
    newdata = train_data_factorized[-train_idx, ],
    type = "response"
  ) > 0.7293,
  1,
  0
)
```

```{r}
pROC::roc(
  train_data_factorized[-train_idx, ]$Survived,
  predictions,
)
```

```{r}
plot(
  pROC::roc(
  train_data_factorized[-train_idx, ]$Survived,
  predictions,
  )
)
```

```{r}
mean(predictions == train_data_factorized[-train_idx, ]$Survived, na.rm = TRUE )
```

```{r}
predictions_test <- ifelse(
  predict(
    sel_stepwise,
    newdata = test_data_factorized,
    type = "response"
  ) > 0.7293,
  1,
  0
)
```


```{r}
submission <- data.frame(
  PassengerId = test_data %>%
    select(PassengerId),
  Survived = predictions_test
)
```

```{r}
submission
```


```{r}
write.csv(
  submission,
  "data/submission.csv",
  row.names = FALSE
)
```

# Submission result:

> ___Score___: 0.6244

This result is awful: as expected, the _logistic regression_ is highly interpretable but suffers from high bias and therefore has low prediction capabilities.

```{r}
convert_to_prob <- function(logit) {
  exp(logit)/(1+exp(logit))
}
```


```{r}
sel_stepwise %>% coef() %>% convert_to_prob()
```

