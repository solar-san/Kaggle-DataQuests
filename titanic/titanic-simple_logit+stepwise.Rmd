---
title: "Machine_Learning_from_Disaster_with_R"
subtitle: "Logit Approach"
author: "Marco Solari"
date: "2023-09-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE
  )

requirements <- c(
  "tidyverse",
  "glmnet",
  "ggthemes"
)

lapply(
  requirements, 
  library, 
  character.only = T)

theme_set(
  theme_tufte(
    base_size = 12,
    base_family = 'Atkinson Hyperlegible'
    )
  )
```

# Loading data

```{r data loader}
data_loader <- function(filename, dataset, path = "data/") {
  
  path <- paste0(path, filename)

  read.csv(
    unz(
      path,
      dataset
    )
  ) 

}
```

```{r}
train_data <- data_loader(
  "titanic.zip",
  "train.csv"
)
test_data <- data_loader(
  "titanic.zip",
  "test.csv"
)
```

# Data Exploration

```{r}
train_data %>% str
```
```{r}
test_data %>% str
```

> In the test data we do not have the dependent variable (and how the f***k am I supposed to evaluate my model? Why Kaggle, why?)

We need to find a way around this:

```{r}
N_train_obs <- dim(train_data)[1]

train_idx <- sample(
  N_train_obs,
  N_train_obs*.5
)
```

# Data transformation: `as.factor`

```{r}
factorize_data <- function(dataframe) {
  dataframe %>% 
  select(-Name, -Ticket) %>% 
  mutate(
    Pclass = as.factor(Pclass),
    Sex = as.factor(Sex),
    SibSp = as.factor(SibSp),
    Parch = as.factor(Parch),
    Cabin = as.factor(Cabin),
    Embarked = as.factor(Embarked)
  )
}
```

```{r}
train_data_factorized <- factorize_data(dataframe = train_data) 
test_data_factorized <- factorize_data(dataframe = test_data)
```

```{r}
train_data_factorized %>% summary
```
```{r, eval=FALSE}
list(
  "train_data_NA" = train_data_factorized %>% 
    is.na() %>% 
    sum,
  "test_data_NA" = test_data_factorized %>% 
    is.na() %>% 
    sum
)
```

```{r, eval=FALSE}
train_data_factorized <- train_data_factorized %>% 
 na.omit()
test_data_factorized <- test_data_factorized %>% 
 na.omit()
```

# Model selection: Logistic Regression + Stepwise Selection

```{r}
fit_full <- glm(
  Survived ~ ., 
  data = train_data_factorized, 
  family = "binomial",
  subset = train_idx
)
```

```{r}
fit_full %>% summary
```

```{r}
fit_null <- glm(
  Survived ~ 1, 
  data = train_data_factorized, 
  family = "binomial",
  subset = train_idx
)
```

```{r}
sel_stepwise <- step(
  fit_full,
  scope = c(fit_null, fit_full),
  direction = "both",
  k = log(
    dim(train_data_factorized)[1]
    )
)
```

```{r}
sel_stepwise <- update(
  sel_stepwise,
  formula = Survived ~ Pclass*Sex*Age
)
sel_stepwise %>% summary()
```

# Make Predictions and Submission `csv`

```{r}
predictions <- 
  ifelse(
  predict(
    sel_stepwise,
    newdata = train_data_factorized[-train_idx, ],
    type = "response"
  ) > 0.35,
  1,
  0
)
```

```{r}
pROC::roc(
  train_data_factorized[-train_idx, ]$Survived,
  predictions,
)
```

```{r}
plot(
  pROC::roc(
  train_data_factorized[-train_idx, ]$Survived,
  predictions,
  )
)
```

```{r}
mean(predictions == train_data_factorized[-train_idx, ]$Survived, na.rm = TRUE )
```

```{r}
predictions_test <- ifelse(
  predict(
    sel_stepwise,
    newdata = test_data_factorized,
    type = "response"
  ) > 0.75,
  1,
  0
)
```


```{r}
submission <- data.frame(
  PassengerId = test_data %>%
    select(PassengerId),
  Survived = predictions_test
)
```


```{r}
write.csv(
  submission,
  "data/submission.csv",
  row.names = FALSE
)
```

# Submission result:

> ___Score___: 0.622

This result is awful: as expected, the _logistic regression_ is highly interpretable but suffers from high bias and therefore has low prediction capabilities.

The interesting result regards the _coefficients_ of the selected model:

```{r}
# sel_stepwise <- update(
#   sel_stepwise,
#   data = train_data %>% factorize_data()
#   ) %>% summary 
```

```{r}
sel_stepwise %>% coefficients()
```

```{r}
convert_to_prob <- function(logit) {
  exp(logit)/(1+exp(logit))
}
```

```{r}
sel_stepwise %>% coefficients() %>%  convert_to_prob()
```

All else equal, these converted coefficients show the probability of survival associated to class, gender == male and age.